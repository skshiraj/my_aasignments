{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5HExLQrE4ZxR"
   },
   "source": [
    "<h1><font color='blue'> 9E and 9F: Finding the Probability P(Y==1|X)</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4LuKrFzC4ZxV"
   },
   "source": [
    "<h2><font color='Geen'> 9E: Implementing Decision Function of SVM RBF Kernel</font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1wES-wWN4ZxX"
   },
   "source": [
    "<font face=' Comic Sans MS' size=3>After we train a kernel SVM model, we will be getting support vectors and their corresponsing coefficients $\\alpha_{i}$\n",
    "\n",
    "Check the documentation for better understanding of these attributes: \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "<img src='https://i.imgur.com/K11msU4.png' width=500>\n",
    "\n",
    "As a part of this assignment you will be implementing the ```decision_function()``` of kernel SVM, here decision_function() means based on the value return by ```decision_function()``` model will classify the data point either as positive or negative\n",
    "\n",
    "Ex 1: In logistic regression After traning the models with the optimal weights $w$ we get, we will find the value $\\frac{1}{1+\\exp(-(wx+b))}$, if this value comes out to be < 0.5 we will mark it as negative class, else its positive class\n",
    "\n",
    "Ex 2: In Linear SVM After traning the models with the optimal weights $w$ we get, we will find the value of $sign(wx+b)$, if this value comes out to be -ve we will mark it as negative class, else its positive class.\n",
    "\n",
    "Similarly in Kernel SVM After traning the models with the coefficients $\\alpha_{i}$ we get, we will find the value of \n",
    "$sign(\\sum_{i=1}^{n}(y_{i}\\alpha_{i}K(x_{i},x_{q})) + intercept)$, here $K(x_{i},x_{q})$ is the RBF kernel. If this value comes out to be -ve we will mark $x_{q}$ as negative class, else its positive class.\n",
    "\n",
    "RBF kernel is defined as: $K(x_{i},x_{q})$ = $exp(-\\gamma ||x_{i} - x_{q}||^2)$\n",
    "\n",
    "For better understanding check this link: https://scikit-learn.org/stable/modules/svm.html#svm-mathematical-formulation\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z830CfMk4Zxa"
   },
   "source": [
    "## Task E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MuBxHiCQ4Zxc"
   },
   "source": [
    "> 1. Split the data into $X_{train}$(60), $X_{cv}$(20), $X_{test}$(20)\n",
    "\n",
    "> 2. Train $SVC(gamma=0.001, C=100.)$ on the ($X_{train}$, $y_{train}$)\n",
    "\n",
    "> 3. Get the decision boundry values $f_{cv}$ on the $X_{cv}$ data  i.e. ` `$f_{cv}$ ```= decision_function(```$X_{cv}$```)```  <font color='red'>you need to implement this decision_function()</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tHie1zqH4Zxt"
   },
   "source": [
    "### Pseudo code\n",
    "\n",
    "clf = SVC(gamma=0.001, C=100.)<br>\n",
    "clf.fit(Xtrain, ytrain)\n",
    "\n",
    "<font color='green'>def</font> <font color='blue'>decision_function</font>(Xcv, ...): #use appropriate parameters <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color='green'>for</font> a data point $x_q$ <font color='green'>in</font> Xcv: <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color='grey'>#write code to implement $(\\sum_{i=1}^{\\text{all the support vectors}}(y_{i}\\alpha_{i}K(x_{i},x_{q})) + intercept)$, here the values $y_i$, $\\alpha_{i}$, and $intercept$ can be obtained from the trained model</font><br>\n",
    "   <font color='green'>return</font> <font color='grey'><i># the decision_function output for all the data points in the Xcv</i></font>\n",
    "    \n",
    "fcv = decision_function(Xcv, ...)  <i># based on your requirement you can pass any other parameters </i>\n",
    "\n",
    "<b>Note</b>: Make sure the values you get as fcv, should be equal to outputs of clf.decision_function(Xcv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fCgMNEvI4Zxf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ANUNIqCe4Zxn"
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=5000, n_features=5, n_redundant=2,\n",
    "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Split the data into  𝑋𝑡𝑟𝑎𝑖𝑛 (60),  𝑋𝑐𝑣 (20),  𝑋𝑡𝑒𝑠𝑡 (20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y)\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train, test_size=0.20, stratify=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 .Train  𝑆𝑉𝐶(𝑔𝑎𝑚𝑚𝑎=0.001,𝐶=100.)  on the ( 𝑋𝑡𝑟𝑎𝑖𝑛 ,  𝑦𝑡𝑟𝑎𝑖𝑛 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SVC(gamma = 0.001,C = 100)\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Get the decision boundry values  𝑓𝑐𝑣  on the  𝑋𝑐𝑣  data i.e.  𝑓𝑐𝑣  = decision_function( 𝑋𝑐𝑣 ) you need to implement this decision_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "coefficient = model.dual_coef_\n",
    "# coefficient = list(coefficient[0])\n",
    "supp_vectors = model.support_vectors_\n",
    "model_implementation = model.decision_function(X_cv)\n",
    "print(type(model_implementation))\n",
    "intercept = model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "from numpy.linalg import norm\n",
    "def decision_function(X_cv,gamma,intercept):\n",
    "    f_cv = []\n",
    "    for x_q in range(len(X_cv)):\n",
    "        s = 0\n",
    "        for x_i in range(len(supp_vectors)):\n",
    "            #s1 = (supp_vectors[x_i] - X_cv[x_q])\n",
    "            s1 = np.linalg.norm(supp_vectors[x_i] - X_cv[x_q])\n",
    "            p = np.exp((- gamma) * (s1 ** 2))  \n",
    "            val = coefficient[0][x_i] * p # coefficient = model.dual_coef_\n",
    "            s = s + val\n",
    "        final_s = s + intercept\n",
    "        \n",
    "        f_cv.append(final_s)\n",
    "    return f_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.001\n",
    "intercept, = model.intercept_ # https://stackoverflow.com/questions/33161448/getting-only-element-from-a-single-element-list-in-python/33161467\n",
    "\n",
    "f_cv =decision_function(X_cv,gamma,intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The custom implementation of decision_function is CORRECT\n"
     ]
    }
   ],
   "source": [
    "f_cv = np.array(f_cv) # converting f_cv as ndarray similar to model_implementation for comparing f_cv and model_implementation \n",
    "\n",
    "# https://stackoverflow.com/questions/34472814/use-a-any-or-a-all\n",
    "if f_cv.all() == model_implementation.all():\n",
    "    print(\"The custom implementation of decision_function is CORRECT\")\n",
    "else:\n",
    "    print(\"The custom implementation of decision_function is WRONG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c0bKCboN4Zxu"
   },
   "source": [
    "<h2><font color='Geen'> 9F: Implementing Platt Scaling to find P(Y==1|X)</font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nMn7OEN94Zxw"
   },
   "source": [
    "Check this <a href='https://drive.google.com/open?id=133odBinMOIVb_rh_GQxxsyMRyW-Zts7a'>PDF</a>\n",
    "<img src='https://i.imgur.com/CAMnVnh.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e0n5EFkx4Zxz"
   },
   "source": [
    "## TASK F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t0HOqVJq4Zx1"
   },
   "source": [
    "\n",
    "> 4. Apply SGD algorithm with ($f_{cv}$, $y_{cv}$) and find the weight $W$ intercept $b$ ```Note: here our data is of one dimensional so we will have a one dimensional weight vector i.e W.shape (1,)``` \n",
    "\n",
    "> Note1: Don't forget to change the values of $y_{cv}$ as mentioned in the above image. you will calculate y+, y- based on data points in train data\n",
    "\n",
    "> Note2: the Sklearn's SGD algorithm doesn't support the real valued outputs, you need to use the code that was done in the `'Logistic Regression with SGD and L2'` Assignment after modifying loss function, and use same parameters that used in that assignment.\n",
    "<img src='https://i.imgur.com/zKYE9Oc.png'>\n",
    "if Y[i] is 1, it will be replaced with y+ value else it will replaced with y- value\n",
    "\n",
    "> 5. For a given data point from $X_{test}$, $P(Y=1|X) = \\frac{1}{1+exp(-(W*f_{test}+ b))}$ where ` `$f_{test}$ ```= decision_function(```$X_{test}$```)```, W and b will be learned as metioned in the above step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oTY7z2bd4Zx2"
   },
   "source": [
    "__Note: in the above algorithm, the steps 2, 4 might need hyper parameter tuning, To reduce the complexity of the assignment we are including the hyerparameter tuning part, but intrested students can try that__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CM3odN1Z4Zx3"
   },
   "source": [
    "\n",
    "If any one wants to try other calibration algorithm istonic regression also please check these tutorials\n",
    "\n",
    "1. http://fa.bianp.net/blog/tag/scikit-learn.html#fn:1\n",
    "\n",
    "2. https://drive.google.com/open?id=1MzmA7QaP58RDzocB0RBmRiWfl7Co_VJ7\n",
    "\n",
    "3. https://drive.google.com/open?id=133odBinMOIVb_rh_GQxxsyMRyW-Zts7a\n",
    "\n",
    "4. https://stat.fandom.com/wiki/Isotonic_regression#Pool_Adjacent_Violators_Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Apply SGD algorithm with ( 𝑓𝑐𝑣 ,  𝑦𝑐𝑣 ) and find the weight  𝑊  intercept  𝑏  Note: here our data is of one dimensional so we will have a one dimensional weight vector i.e W.shape (1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding number of 0's and 1's in y_cv \n",
    "N_pos = 0\n",
    "N_neg = 0\n",
    "for i in range(len(y_cv)):\n",
    "    if y_cv[i] == 1:\n",
    "        N_pos += 1\n",
    "    else:\n",
    "        N_neg += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9959016393442623"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Caculating y+ value for y_cv \n",
    "val_pos = (N_pos + 1)/(N_pos + 2)\n",
    "val_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0017857142857142857"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Caculating y- value for y_cv \n",
    "val_neg = 1/(N_neg + 2)\n",
    "val_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ycv_copy = np.copy(y_cv)\n",
    "x_pos = [] # values corresponds to y+ label\n",
    "x_neg = [] # values corresponds to y- label\n",
    "\n",
    "# https://stackoverflow.com/questions/1614236/in-python-how-do-i-convert-all-of-the-items-in-a-list-to-floats\n",
    "ycv_copy = [float(i) for i in ycv_copy]\n",
    "for i in range(len(ycv_copy)):\n",
    "    if ycv_copy[i] == 1:\n",
    "        ycv_copy[i] = val_pos\n",
    "        x_pos.append(f_cv[i])\n",
    "    else:\n",
    "        ycv_copy[i] = val_neg\n",
    "        x_neg.append(f_cv[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pos = np.array(x_pos)\n",
    "x_neg = np.array(x_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([1]) # initializing w of shape(1,)\n",
    "b = 0\n",
    "eta0  = 0.0001\n",
    "alpha = 0.0001\n",
    "n1 = len(f_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(w,xi,b):\n",
    "    a = 1/(1 + np.exp(-(np.dot(xi,w.transpose()) + b)))\n",
    "    return a\n",
    "    \n",
    "def log_loss(w,b,f_cv,y_cv):\n",
    "    N = len(f_cv)\n",
    "    sum_log = 0\n",
    "    for i in range(len(x_pos)):\n",
    "        w = (1 - (alpha*lambd)/N) * w + (alpha * x_pos[i] *(val_pos - sigmoid(w,x_pos[i],b))) # statement 1\n",
    "        b = (b + alpha * (val_pos - sigmoid(w,x_pos[i],b)))\n",
    "        sum_log = sum_log + (val_pos * np.log10(sigmoid(w,x_pos[i],b))) + (1 - val_pos) * np.log10(1-sigmoid(w,x_pos[i],b))\n",
    "    for i in range(len(x_neg)):\n",
    "        w = (1 - (alpha*lambd)/N) * w + (alpha * x_neg[i] *(val_neg - sigmoid(w,x_neg[i],b))) # statement 1\n",
    "        b = (b + alpha * (val_neg - sigmoid(w,x_neg[i],b)))\n",
    "        sum_log = sum_log + (val_neg * np.log10(sigmoid(w,x_neg[i],b))) + (1 - val_neg) * np.log10(1-sigmoid(w,x_neg[i],b))\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambd = 1 # value of lambda\n",
    "w,b = log_loss(w,b,f_cv,y_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n",
      "[-0.00174054]\n"
     ]
    }
   ],
   "source": [
    "print(w.shape)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) For a given data point from  𝑋𝑡𝑒𝑠𝑡 ,  𝑃(𝑌=1|𝑋)=11+𝑒𝑥𝑝(−(𝑊∗𝑓𝑡𝑒𝑠𝑡+𝑏))  where  𝑓𝑡𝑒𝑠𝑡  = decision_function( 𝑋𝑡𝑒𝑠𝑡 ), W and b will be learned as metioned in the above step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.001\n",
    "intercept, = b\n",
    "f_test = decision_function(X_test,gamma,intercept) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.35240664e-02 1.39960709e-02 3.42698269e-01 9.25798547e-03\n",
      " 3.40241352e-02 2.64215735e-02 8.52173807e-03 5.55296894e-01\n",
      " 1.27390544e-02 9.27037407e-02 9.04852491e-02 4.81790340e-01\n",
      " 3.20116238e-02 1.93646207e-03 5.93117829e-01 1.72343956e-02\n",
      " 2.02098071e-01 6.86193451e-02 8.98619537e-02 2.30477598e-02\n",
      " 8.02846345e-03 6.35575446e-03 5.32320439e-01 4.29917456e-02\n",
      " 6.93451377e-01 8.07794123e-02 4.66833312e-03 1.80421240e-02\n",
      " 2.03294803e-01 2.51018937e-01 2.61563734e-02 3.03511365e-01\n",
      " 5.07689425e-01 9.65510238e-03 3.87311326e-02 1.53309139e-02\n",
      " 2.07259959e-01 1.29812824e-02 5.01041098e-01 3.75317214e-01\n",
      " 1.70278319e-03 2.61121088e-02 3.40603844e-02 9.55586594e-02\n",
      " 1.00269182e-01 2.73247737e-03 4.13099535e-02 2.43277299e-02\n",
      " 5.16381961e-01 7.40361737e-01 5.53297316e-01 5.46979359e-01\n",
      " 5.70015031e-03 1.24552617e-02 2.97887676e-01 2.06164817e-02\n",
      " 6.26874887e-01 1.75909345e-02 2.92205810e-02 4.66821642e-03\n",
      " 3.19751010e-01 8.21375769e-01 1.24759974e-02 5.80592575e-01\n",
      " 5.12775977e-01 8.66477004e-02 4.35086366e-02 4.94926742e-02\n",
      " 5.06650736e-01 6.07962581e-01 4.80837179e-01 3.03024930e-01\n",
      " 5.42171202e-03 1.94190797e-02 2.97039647e-02 4.95045113e-01\n",
      " 7.91855672e-03 6.83164764e-02 1.19330605e-02 6.94777885e-01\n",
      " 1.50809305e-02 2.19768451e-02 9.10261061e-03 5.14427313e-01\n",
      " 3.53173337e-01 1.45326448e-02 1.16343101e-02 3.95907745e-01\n",
      " 7.76720761e-03 2.18332260e-01 1.11081919e-02 4.18326741e-01\n",
      " 3.59905984e-02 8.75782459e-02 2.09802592e-03 4.64259334e-03\n",
      " 3.97641181e-03 3.24335503e-02 6.94350649e-01 5.43127280e-02\n",
      " 3.37883973e-02 4.64939751e-01 5.65091717e-01 1.55208641e-02\n",
      " 5.03775458e-01 7.20559506e-03 2.37891701e-01 7.16315235e-03\n",
      " 1.20072587e-02 4.90226943e-01 4.85603087e-01 1.60967704e-02\n",
      " 6.48509768e-01 1.37546480e-02 2.64778670e-02 9.59014818e-02\n",
      " 1.64759800e-02 1.25954779e-02 2.37813435e-02 1.01948987e-02\n",
      " 9.04137084e-04 8.89600687e-01 5.58110308e-04 5.94920430e-01\n",
      " 2.97619500e-02 1.85443917e-02 2.51914103e-02 1.99880822e-02\n",
      " 3.89688621e-01 3.70329763e-01 1.61639612e-01 1.77745973e-02\n",
      " 5.99330611e-02 7.45901066e-03 1.54544822e-02 5.15060820e-01\n",
      " 5.35867461e-01 3.85764727e-02 1.84089021e-02 5.58431892e-01\n",
      " 3.60396369e-02 5.32412839e-01 9.64282068e-03 1.74243171e-03\n",
      " 7.04631425e-01 1.08725438e-02 1.84829328e-02 1.51257071e-01\n",
      " 5.45015104e-01 1.00727685e-02 3.00844095e-01 1.97465290e-02\n",
      " 9.33128637e-03 3.23411066e-01 4.84944447e-01 3.93470416e-02\n",
      " 1.28487035e-02 3.80149840e-03 1.55389955e-02 6.79927582e-03\n",
      " 1.92190175e-02 5.18247250e-01 1.03181860e-02 2.68124493e-02\n",
      " 8.42970491e-01 3.71514073e-01 1.17425156e-01 5.80721440e-01\n",
      " 1.36592736e-02 3.92747033e-02 2.00599651e-02 4.49496295e-01\n",
      " 1.19572881e-02 4.85965067e-02 4.23686007e-03 1.21726090e-03\n",
      " 8.81159151e-03 1.18063814e-02 1.53462355e-02 3.54304475e-02\n",
      " 1.53941381e-02 6.86210181e-03 9.48086185e-03 1.98328814e-02\n",
      " 5.79905681e-01 2.07747528e-02 3.77182790e-03 4.75795519e-01\n",
      " 2.76965088e-01 2.24096934e-03 6.83467314e-02 1.98659239e-02\n",
      " 5.88819285e-03 3.72222473e-02 3.51541032e-01 8.00340568e-03\n",
      " 7.96184018e-03 5.38682661e-02 1.51700218e-02 4.60769744e-01\n",
      " 9.67897267e-03 4.35494035e-01 1.25402216e-02 5.28772433e-01\n",
      " 1.13812107e-01 8.59745847e-02 5.01016677e-01 4.23431526e-02\n",
      " 5.32360847e-03 1.04841050e-02 1.32687349e-02 7.43232879e-03\n",
      " 2.10070062e-02 5.57208558e-03 4.21102875e-02 7.21596145e-02\n",
      " 4.60156112e-03 1.09748804e-02 7.57727889e-03 4.28155918e-01\n",
      " 9.07416225e-03 3.40216771e-02 1.83381213e-02 1.03375352e-02\n",
      " 8.21214167e-02 3.05781439e-02 1.24695832e-02 3.60264717e-01\n",
      " 6.65556592e-03 5.65376738e-02 3.14127567e-01 5.43892074e-02\n",
      " 1.35398383e-01 1.02628138e-01 9.30711931e-03 7.00966254e-02\n",
      " 5.86142886e-02 4.79866514e-01 8.17019816e-04 1.81509578e-01\n",
      " 2.60681166e-03 1.31047682e-02 6.76958127e-02 1.18079341e-02\n",
      " 2.01177201e-02 8.76027757e-02 9.07235082e-03 8.24869168e-02\n",
      " 4.48891050e-02 8.84336476e-02 2.40921412e-02 4.77978132e-01\n",
      " 5.15485792e-02 6.12808725e-01 3.72618230e-02 2.70663359e-02\n",
      " 2.65104707e-02 3.17006831e-02 6.87194775e-01 4.38272189e-01\n",
      " 6.29704492e-02 7.92567124e-01 7.22392021e-01 9.59801910e-03\n",
      " 6.29629539e-02 1.28977565e-02 1.47232401e-02 3.08227077e-03\n",
      " 5.51720985e-01 1.64732557e-02 3.89860087e-02 4.85582111e-01\n",
      " 4.36698516e-01 2.39517995e-02 3.05414569e-02 1.99006977e-02\n",
      " 1.90873165e-02 5.28075804e-03 5.98298718e-02 3.73364900e-02\n",
      " 2.26407017e-02 3.35703402e-02 7.39600167e-02 1.30854094e-02\n",
      " 1.86085207e-02 1.35927484e-02 8.11190359e-03 3.58403225e-02\n",
      " 7.28916881e-03 8.48352812e-02 6.29292483e-03 1.73669371e-02\n",
      " 5.65511272e-01 1.50983905e-02 6.80790780e-03 5.49130752e-01\n",
      " 2.74077221e-01 6.12542769e-01 1.25253552e-02 7.65341994e-02\n",
      " 4.17458570e-01 2.56635405e-02 3.74315701e-02 2.57396839e-02\n",
      " 3.18630106e-02 9.51667812e-03 1.41976217e-02 2.69371866e-01\n",
      " 8.78389400e-03 5.52182429e-01 4.99117485e-01 1.90253623e-02\n",
      " 6.54049014e-01 2.67664800e-01 4.77759040e-01 6.91860706e-01\n",
      " 2.71935533e-03 3.49226808e-03 3.78299900e-01 5.37324136e-01\n",
      " 1.69949235e-02 4.30354798e-02 1.91007611e-02 7.05971690e-01\n",
      " 4.45164114e-02 1.33371005e-02 3.90886110e-02 1.35574697e-02\n",
      " 5.81931058e-01 3.25791791e-02 4.26067278e-02 8.05976133e-03\n",
      " 3.16090997e-03 1.14727186e-02 4.43085660e-02 8.69852258e-03\n",
      " 4.14083381e-02 4.93327383e-03 1.07077418e-02 8.89405475e-03\n",
      " 3.82335636e-02 3.56448582e-01 7.36291284e-03 6.70515422e-03\n",
      " 1.61750231e-02 3.80440752e-02 5.21312313e-02 3.19123889e-03\n",
      " 4.78171637e-02 2.49484318e-01 1.68202430e-01 5.56581062e-01\n",
      " 6.32384555e-02 1.74894492e-02 5.37656148e-01 1.49661608e-02\n",
      " 4.56248992e-02 9.15294111e-03 3.90925917e-02 1.05654411e-01\n",
      " 1.64973619e-02 5.51961661e-01 2.30354801e-02 4.96776998e-01\n",
      " 1.02996937e-02 3.02154648e-01 6.69277044e-01 2.47091628e-01\n",
      " 2.72067110e-03 5.92159796e-01 8.79248331e-03 3.98105166e-02\n",
      " 1.95165607e-02 5.59559561e-03 4.51440382e-03 1.57966308e-01\n",
      " 2.65563459e-01 5.50692966e-01 5.69084223e-03 2.00713738e-02\n",
      " 3.13108587e-01 1.04673588e-02 8.72720993e-02 4.50226086e-01\n",
      " 1.26285411e-02 2.91766265e-02 1.35504434e-02 4.18086644e-02\n",
      " 6.26278537e-01 1.45531284e-01 5.96913126e-01 6.91399961e-02\n",
      " 1.35591403e-02 9.15798861e-03 1.76248480e-02 4.69181432e-01\n",
      " 8.04732033e-03 7.26741119e-03 2.51742436e-01 1.08502910e-02\n",
      " 1.83741135e-03 8.15262905e-03 5.01847268e-03 4.35454457e-02\n",
      " 1.55968318e-02 4.31997498e-02 1.43723189e-01 9.32237584e-03\n",
      " 8.39602987e-03 1.28159784e-01 9.49752100e-03 5.49703389e-01\n",
      " 2.47929086e-02 1.91212756e-02 1.27078389e-02 1.67513441e-02\n",
      " 8.23341584e-03 4.71866662e-01 5.43108066e-01 2.90148620e-02\n",
      " 1.51692469e-02 8.43590375e-03 2.74241620e-02 1.71235346e-02\n",
      " 8.03668321e-03 1.35079438e-01 4.66385133e-01 1.77162155e-01\n",
      " 2.24930670e-01 6.04186083e-02 4.41742498e-01 4.32709759e-02\n",
      " 5.47190815e-03 2.59652248e-02 1.26400358e-02 1.06189820e-01\n",
      " 5.34292740e-01 5.63820409e-01 3.69586388e-03 5.46206863e-03\n",
      " 1.28634027e-01 4.37770533e-03 5.26464964e-02 5.68422210e-01\n",
      " 3.45833641e-02 5.13517829e-03 3.41755194e-02 8.31340067e-03\n",
      " 5.50117952e-01 5.42500066e-01 3.34127236e-01 3.56282852e-01\n",
      " 2.02696778e-02 1.76139436e-01 6.08704582e-01 3.48867445e-02\n",
      " 8.16014661e-01 4.76312606e-01 2.62321078e-03 5.63814867e-01\n",
      " 1.04676192e-02 1.87291241e-01 5.09134710e-03 7.17852185e-03\n",
      " 3.36494241e-01 3.16197595e-01 1.68083111e-02 2.38034186e-02\n",
      " 1.57018994e-02 8.32809135e-03 5.65222984e-03 4.46737800e-01\n",
      " 4.45252848e-02 1.48671606e-02 4.38395160e-01 1.40213179e-01\n",
      " 1.55185885e-02 6.95249176e-03 6.41263454e-01 5.12327124e-01\n",
      " 1.70268722e-02 5.56028420e-03 4.37681813e-01 1.55813870e-02\n",
      " 4.19369995e-02 9.56532651e-02 3.69542217e-02 4.68944857e-01\n",
      " 6.31720143e-01 6.62145545e-01 4.83696254e-01 4.79497050e-01\n",
      " 1.98768951e-02 9.61246465e-02 1.25340703e-02 2.54086111e-02\n",
      " 1.37059795e-01 8.72105394e-03 1.52401910e-01 1.74973440e-02\n",
      " 9.07599213e-03 6.00489231e-01 4.54403774e-01 3.64613873e-03\n",
      " 4.09753770e-01 1.44614938e-02 5.25686720e-01 1.94720557e-02\n",
      " 6.39166267e-01 5.88305692e-03 2.12685314e-02 6.57154807e-01\n",
      " 5.56725010e-01 7.93995071e-01 5.07461009e-01 1.13470913e-02\n",
      " 6.21650784e-03 3.08878821e-01 6.03578835e-03 4.24653485e-02\n",
      " 8.66543607e-03 1.44713552e-02 6.78157268e-01 7.89853989e-02\n",
      " 1.13503341e-02 3.88860129e-02 1.28670945e-02 3.80384541e-03\n",
      " 1.23769131e-02 1.88836466e-01 8.80558223e-03 4.21332183e-01\n",
      " 3.32430668e-02 1.63742653e-02 5.38482886e-02 5.52749377e-01\n",
      " 5.83955866e-01 2.56374170e-02 1.39752379e-01 2.58474974e-01\n",
      " 2.11786320e-02 2.73059817e-03 4.34097726e-01 1.95823529e-02\n",
      " 2.34225930e-02 8.62881140e-02 5.78797953e-01 3.18796622e-01\n",
      " 3.16105119e-02 8.34546923e-03 1.69018376e-02 7.56050508e-03\n",
      " 4.82602893e-01 1.84479250e-02 5.28155698e-01 3.83808554e-01\n",
      " 2.21577517e-01 4.92582847e-01 1.70994950e-02 8.08411197e-01\n",
      " 1.48195904e-02 9.97512782e-03 8.51910020e-01 1.85133853e-02\n",
      " 1.28435830e-01 4.79499601e-01 2.17287054e-02 1.21872249e-02\n",
      " 1.04104627e-02 7.90876845e-02 1.85762444e-02 7.16576645e-01\n",
      " 1.75444023e-02 8.82390463e-02 4.62693133e-02 2.35234851e-02\n",
      " 1.39003912e-02 5.96754893e-01 3.86024589e-02 1.00509101e-02\n",
      " 7.88461328e-03 4.51316323e-03 3.17643362e-02 8.64753676e-02\n",
      " 5.29383454e-02 2.34217030e-01 5.95478511e-01 2.48816228e-02\n",
      " 4.13872543e-01 2.27691078e-02 5.42113739e-01 2.13302401e-02\n",
      " 9.10187054e-01 1.77788182e-02 2.24699981e-02 1.44263829e-02\n",
      " 9.18440639e-03 1.51226566e-01 2.70308277e-03 3.67276556e-01\n",
      " 2.10127375e-02 8.96955056e-03 5.13353484e-01 6.07856750e-01\n",
      " 4.65393233e-01 1.15960277e-02 5.79552016e-03 7.32126770e-03\n",
      " 1.78621486e-01 6.15727638e-03 1.49835701e-02 6.36830582e-01\n",
      " 4.44312212e-02 1.24677702e-01 1.56877012e-02 6.96346444e-01\n",
      " 5.06939863e-03 6.13013178e-01 1.55416470e-02 4.20807202e-01\n",
      " 1.25400286e-02 1.80064884e-02 9.36577946e-03 8.85212582e-03\n",
      " 5.20634677e-01 2.62338928e-01 1.61645708e-01 4.06475649e-02\n",
      " 4.99175521e-03 3.58864352e-01 7.02071064e-03 4.60190707e-01\n",
      " 1.28598457e-02 9.83200778e-03 8.24601347e-03 3.14807850e-02\n",
      " 1.93158173e-02 3.37270573e-01 2.61551535e-01 1.71662635e-02\n",
      " 3.60836388e-02 3.50269204e-02 5.56474536e-01 2.33138688e-01\n",
      " 3.88219001e-02 5.99205786e-01 1.63578731e-02 7.50773921e-03\n",
      " 4.74425420e-01 3.36347360e-02 2.00850632e-02 1.28969003e-01\n",
      " 3.06332284e-01 9.53642570e-03 8.27148788e-02 6.13815515e-01\n",
      " 2.88418421e-02 2.12476431e-03 5.48296197e-01 4.41458125e-01\n",
      " 4.04595654e-02 4.00663535e-02 7.95732306e-03 1.24455423e-01\n",
      " 6.60296137e-02 1.23155351e-02 2.81709264e-02 4.16635470e-03\n",
      " 8.16316815e-03 7.75984061e-01 2.16594886e-03 6.81882847e-03\n",
      " 1.88363372e-01 1.75732201e-01 1.78065986e-02 1.34721619e-02\n",
      " 5.65706067e-03 2.85033144e-02 6.13707894e-01 3.08254197e-02\n",
      " 1.13872022e-03 1.91274388e-01 5.71678740e-01 1.32486940e-02\n",
      " 6.56188277e-01 7.27896758e-03 2.22864721e-03 6.21800143e-02\n",
      " 5.44015443e-01 1.90281971e-02 6.84666223e-02 1.73712284e-02\n",
      " 4.13628556e-01 8.47162151e-03 1.08498024e-02 1.11159393e-02\n",
      " 9.84008193e-03 5.25790295e-01 6.43444825e-01 9.36306622e-03\n",
      " 3.19500109e-02 4.05441404e-01 3.70820243e-03 5.33178542e-01\n",
      " 1.74329617e-01 3.41184434e-02 4.72353986e-01 5.55105897e-01\n",
      " 6.23097788e-01 1.46185176e-02 2.25788333e-03 2.91686386e-02\n",
      " 2.96490439e-03 2.41955224e-02 6.60518938e-01 1.54764311e-02\n",
      " 3.37080428e-02 7.38107688e-03 1.30027525e-01 2.86696892e-02\n",
      " 1.05615636e-02 1.16319026e-02 1.63889576e-02 1.00800839e-02\n",
      " 3.55473461e-03 1.51015920e-01 4.06215738e-02 5.28008206e-01\n",
      " 7.06941743e-03 6.75882347e-03 2.11603360e-02 9.82788842e-03\n",
      " 1.04491937e-02 5.46371749e-01 5.85300594e-02 6.82699482e-02\n",
      " 4.98811499e-01 4.66418558e-01 4.35454294e-03 1.06417804e-02\n",
      " 3.60767415e-04 1.52599508e-02 5.29072284e-01 1.44995124e-02\n",
      " 8.12326701e-01 6.20059489e-02 4.82789177e-03 5.95033740e-01\n",
      " 7.37380737e-02 1.91639963e-01 1.01081005e-02 1.26675595e-02\n",
      " 3.31046040e-01 3.72966142e-03 8.93081562e-01 1.91157714e-02\n",
      " 8.53741131e-03 8.36249653e-02 3.21305747e-03 4.38732574e-01\n",
      " 4.33229160e-03 2.66207856e-02 4.43187881e-01 7.18786088e-02\n",
      " 3.09086537e-02 4.15429692e-01 6.19038337e-02 6.45984211e-01\n",
      " 5.46251150e-03 2.54561210e-02 9.04473518e-02 4.90282938e-01\n",
      " 5.89913321e-01 3.27886976e-02 6.90997838e-03 1.67249568e-02\n",
      " 1.13098913e-02 1.77149494e-02 4.96892163e-01 7.95920420e-03\n",
      " 5.63747000e-03 2.62234095e-03 7.88751659e-02 1.01791544e-02\n",
      " 3.14857160e-03 3.27287632e-03 1.07619713e-02 5.11277546e-01\n",
      " 2.68871671e-02 2.03289623e-02 8.14631249e-03 9.76808840e-02\n",
      " 4.40015015e-01 4.03337067e-03 1.17368956e-02 5.28334415e-03\n",
      " 4.39637171e-02 1.03859758e-01 6.61308396e-03 6.26998572e-01\n",
      " 1.44234901e-01 1.07424056e-01 4.64794502e-01 1.19496819e-01\n",
      " 1.77313048e-02 1.12306181e-02 2.35328914e-02 5.33110994e-03\n",
      " 2.44676760e-01 2.61498512e-01 2.58159544e-02 1.05741758e-01\n",
      " 2.96405972e-03 9.90619104e-02 5.87957444e-01 1.84903902e-01\n",
      " 6.16078718e-01 2.04468412e-02 2.54168194e-02 4.51035416e-01\n",
      " 1.83763008e-02 4.33250283e-02 3.07556047e-02 5.55081887e-02\n",
      " 8.68352197e-03 2.42067796e-02 2.55276163e-02 1.61367160e-01\n",
      " 6.12764485e-02 4.73515463e-01 1.52307357e-03 3.02146346e-02\n",
      " 1.47437635e-02 6.38632457e-03 2.94653481e-02 1.30328399e-02\n",
      " 1.04857147e-02 7.80916465e-03 3.87346597e-02 6.28646583e-01\n",
      " 2.87637724e-02 6.71264194e-03 1.53325379e-02 5.90152755e-01\n",
      " 3.24033581e-02 6.91201204e-02 4.54701141e-01 5.71215829e-02\n",
      " 4.98139239e-03 1.08146629e-02 1.70212214e-02 1.15468660e-02\n",
      " 2.91631802e-03 1.08802157e-02 1.87433684e-02 1.02423753e-01\n",
      " 1.90399282e-02 1.81780021e-02 4.00465942e-03 4.06822742e-03\n",
      " 1.92577328e-01 2.67455127e-02 5.04006690e-01 1.20387524e-01\n",
      " 3.37803564e-02 1.59969153e-02 1.22691341e-02 3.71600483e-02\n",
      " 6.86376163e-01 5.17325054e-02 1.82921315e-03 3.80089011e-02\n",
      " 1.09984679e-02 2.11553775e-02 5.45581315e-01 1.60580288e-02\n",
      " 2.50695219e-02 6.54124813e-02 2.69598807e-02 1.40619080e-02\n",
      " 2.11042284e-03 4.99482460e-03 5.45962018e-01 1.18800049e-02\n",
      " 4.44805967e-03 2.10273299e-01 2.72096048e-01 1.68907447e-02\n",
      " 5.09741064e-03 1.36922300e-01 1.09944447e-02 3.18975560e-01\n",
      " 3.44163637e-01 1.44517297e-02 4.44176796e-01 3.42408808e-02\n",
      " 1.01078762e-01 1.57907349e-02 1.83696147e-01 1.44059735e-02\n",
      " 6.87906095e-02 2.20417543e-02 4.84063977e-01 5.59157736e-01\n",
      " 5.43741884e-03 8.47656879e-03 5.27162108e-01 3.34980761e-01\n",
      " 2.15917729e-02 4.73148176e-01 1.17339094e-02 5.24500356e-02\n",
      " 2.82682718e-02 1.65758167e-02 1.81411745e-01 1.39454862e-02\n",
      " 9.54812651e-02 6.28435764e-01 1.29935302e-02 8.99125106e-03\n",
      " 1.16587135e-01 8.72154373e-02 4.38477321e-01 5.53872965e-03\n",
      " 6.25931726e-01 3.61278442e-02 1.52179376e-02 3.99911285e-03\n",
      " 2.18473167e-02 3.46111534e-02 6.57094248e-01 6.85781739e-03\n",
      " 5.48248283e-02 3.31826562e-01 5.32465295e-01 4.15046838e-02\n",
      " 1.15570555e-02 4.93962725e-01 1.15537835e-01 1.75578503e-02\n",
      " 7.41890533e-01 1.80189013e-02 6.03420398e-02 1.22535430e-02\n",
      " 2.06797706e-01 8.83237210e-03 5.00701772e-03 2.37717103e-02\n",
      " 4.70642245e-02 2.01257907e-02 1.57937600e-02 8.35156330e-01\n",
      " 3.30273459e-01 1.06248252e-02 7.96779252e-03 4.81041005e-01\n",
      " 4.56796119e-01 6.06336612e-02 1.56288243e-03 6.90604658e-01\n",
      " 8.94501677e-03 2.24169130e-02 1.88066646e-02 5.90291837e-01\n",
      " 2.99540516e-03 2.65354978e-02 5.10249463e-01 5.77050378e-01\n",
      " 9.72393732e-02 6.64792450e-01 6.12437270e-01 7.97286428e-03\n",
      " 1.22079308e-02 1.00438645e-02 1.16267032e-01 8.88934652e-02\n",
      " 5.41047930e-01 5.84863793e-03 1.70977529e-01 3.48229364e-02\n",
      " 1.48814952e-02 5.22056195e-01 1.25259035e-01 5.88158577e-03\n",
      " 1.87218674e-02 1.51978684e-01 1.16014698e-02 5.92311980e-02\n",
      " 3.12773768e-03 8.64636117e-02 8.20993097e-02 5.11138468e-03\n",
      " 6.76177466e-02 2.35620075e-01 5.92763705e-01 3.50650438e-02\n",
      " 5.18959365e-01 1.11503258e-02 2.24723013e-02 2.02962172e-02]\n"
     ]
    }
   ],
   "source": [
    "# Finding Calibrated Probabilities\n",
    "P = 1/(1 + np.exp(-((w * f_test) + b)))\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "8E&F_LR_SVM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
