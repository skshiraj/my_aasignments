{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "x=boston.data #independent variables\n",
    "y=boston.target #target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: 1\n",
    "<font color='red'><b>Step 1 Creating samples: </b></font> Randomly create 30 samples from the whole boston data points.\n",
    "<ol>\n",
    "<li>Creating each sample: Consider any random 303(60% of 506) data points from whole data set and then replicate any 203 points from the sampled points</li>\n",
    "<li>Ex: For better understanding of this procedure lets check this examples, assume we have 10 data points [1,2,3,4,5,6,7,8,9,10], first we take 6 data points randomly consider we have selected [4, 5, 7, 8, 9, 3] now we will replciate 4 points from [4, 5, 7, 8, 9, 3], consder they are [5, 8, 3,7] so our final sample will be [4, 5, 7, 8, 9, 3, 5, 8, 3,7]</li>\n",
    "<li> we create 30 samples like this </li>\n",
    "<li> Note that as a part of the Bagging when you are taking the random samples make sure each of the sample will have                different set of columns</li>\n",
    "<li> Ex: assume we have 10 columns for the first sample we will select [3, 4, 5, 9, 1, 2] and for the second sample [7, 9, 1, 4, 5, 6, 2] and so on...</li>\n",
    "<li> Make sure each sample will have atleast 3 feautres/columns/attributes</li>\n",
    "</ol>\n",
    "\n",
    "<font color='red'><b>Step 2 Building High Variance Models on each of the sample and finding train MSE value:</b></font> Build a DecisionTreeRegressor on each of the sample.\n",
    "<ol><li>Build a regression trees on each of 30 samples.</li>\n",
    "<li>computed the predicted values of each data point(506 data points) in your corpus.</li>\n",
    "<li> predicted house price of $i^{th}$ data point $y^{i}_{pred} =  \\frac{1}{30}\\sum_{k=1}^{30}(\\text{predicted value of } x^{i} \\text{ with } k^{th} \\text{ model})$.</li>\n",
    "<li>Now calculate the $MSE =  \\frac{1}{506}\\sum_{i=1}^{506}(y^{i} - y^{i}_{pred})^{2}$.</li>\n",
    "</ol>\n",
    "\n",
    "<font color='red'><b>Step 3 Calculating the OOB score :</b></font>\n",
    "<ol>\n",
    "<li>Computed the predicted values of each data point(506 data points) in your corpus.</li>\n",
    "<li>Predicted house price of $i^{th}$ data point $y^{i}_{pred} =  \\frac{1}{k}\\sum_{\\text{k= model which was buit on samples not included } x^{i}}(\\text{predicted value of } x^{i} \\text{ with } k^{th} \\text{ model})$.</li>\n",
    "<li>Now calculate the $OOB Score =  \\frac{1}{506}\\sum_{i=1}^{506}(y^{i} - y^{i}_{pred})^{2}$.</li>\n",
    "</ol>\n",
    "\n",
    "### Task: 2\n",
    "<pre>\n",
    "<font color='red'><b>Computing CI of OOB Score and Train MSE</b></font>\n",
    "<ol>\n",
    "<li> Repeat Task 1 for 35 times, and for each iteration store the Train MSE and OOB score </li>\n",
    "<li> After this we will have 35 Train MSE values and 35 OOB scores </li>\n",
    "<li> using these 35 values (assume like a sample) find the confidence intravels of MSE and OOB Score </li>\n",
    "<li> you need to report CI of MSE and CI of OOB Score </li>\n",
    "<li> Note: Refer the Central_Limit_theorem.ipynb to check how to find the confidence intravel</li>\n",
    "</ol>\n",
    "</pre>\n",
    "### Task: 3\n",
    "<pre>\n",
    "<font color='red'><b>Given a single query point predict the price of house.</b></font>\n",
    "\n",
    "<li>Consider xq= [0.18,20.0,5.00,0.0,0.421,5.60,72.2,7.95,7.0,30.0,19.1,372.13,18.60] Predict the house price for this point as mentioned in the step 2 of Task 1. </li>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Step 1) Creating samples:Randomly create 30 samples from the whole boston data points:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for creating 30 samples\n",
    "import random\n",
    "def creating_samples(n1,n2,x):\n",
    "    samples = []\n",
    "    oob_samples = []\n",
    "    final_y_samples = []\n",
    "    indices = [i for i in range(len(x))]\n",
    "    li = [3,5,6,7,8,9,10]\n",
    "    selected_columns = []\n",
    "    for i in range(30):\n",
    "        y_samples = []\n",
    "        l1 = []\n",
    "        index = np.random.choice(x.shape[0], n1, replace=False) # selecting random 60%(303) indices or rows of data points from x : https://stackoverflow.com/questions/43506766/randomly-select-from-numpy-array\n",
    "        for i in range(len(indices)):\n",
    "            if i not in index:\n",
    "                l1.append(i)\n",
    "                \n",
    "        for i in index:\n",
    "            y_samples.append(y[i])\n",
    "            \n",
    "        new_index = np.random.choice(index, n2, replace=True) #  selecting indices 203 indices or rows from randomly selected rows\n",
    "        for i in new_index:\n",
    "            y_samples.append(y[i])\n",
    "            \n",
    "        s = random.randrange(3, 8)\n",
    "        l = random.sample(li, k = s)\n",
    "        \n",
    "        x_r = x[index] # randomly selected 303 data points from the data x\n",
    "        x_r2 = x[new_index] # randomly selected 203 points from 303 data points\n",
    "        \n",
    "        oob_x = x[l1]\n",
    "        \n",
    "        x_r = x_r[:,l] # randomly selection of features/columns \n",
    "        x_r2 = x_r2[:,l]\n",
    "        oob_x = oob_x[:,l]\n",
    "        \n",
    "        selected_columns.append(l) # list of lists in which each list contains feature/column numbers which are used in a particular sample\n",
    "        final_x = np.concatenate((x_r,x_r2),axis = 0) # finaly getting a sample\n",
    "        samples.append(final_x) # storing each sample in list\n",
    "        oob_samples.append(oob_x)\n",
    "        final_y_samples.append(y_samples) # list of lists containing sample labels\n",
    "    return samples,oob_samples,selected_columns,final_y_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 6)\n",
      "30\n",
      "[[7, 8, 6, 3, 10], [8, 9, 6, 7], [7, 6, 5, 8, 10, 9], [10, 9, 6], [8, 7, 3, 6], [10, 7, 5, 8, 3, 6], [6, 5, 7, 10, 8, 9], [8, 9, 3, 5, 7, 6], [10, 8, 6, 9, 5, 7, 3], [3, 7, 9, 6, 8, 5, 10], [8, 6, 5, 9, 3, 7], [7, 10, 5, 6, 3], [3, 10, 9, 8, 5, 7, 6], [5, 3, 7, 8, 9, 10], [9, 8, 10, 3, 5, 6, 7], [3, 9, 7], [5, 9, 6, 10, 7], [9, 6, 10, 8, 7], [5, 3, 6, 8, 9, 7, 10], [3, 6, 8], [3, 9, 5, 7, 10, 8], [10, 8, 9, 7], [10, 8, 6], [7, 3, 5, 10], [7, 10, 5, 3], [7, 6, 10], [3, 6, 8, 7], [5, 8, 3, 10, 6], [5, 8, 7], [8, 7, 5, 10, 9, 6]]\n"
     ]
    }
   ],
   "source": [
    "n1 = 303\n",
    "n2 = 203\n",
    "samples,oob_samples,selected_columns,final_y_samples = creating_samples(n1,n2,x)\n",
    "print(samples[29].shape)\n",
    "print(len(final_y_samples))\n",
    "print(selected_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting each ndarray sample into type list by which we can apply different operations easily\n",
    "for i in range(len(samples)):\n",
    "    samples[i] = samples[i].tolist()\n",
    "    oob_samples[i] = oob_samples[i].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Step 2) Building High Variance Models on each of the sample and finding train MSE value:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from operator import add\n",
    "def pred1(samples,myInt,final_y_samples,selected_columns):\n",
    "    y_pred = []\n",
    "    final_y_pred = []\n",
    "    \n",
    "    for i in range(len(samples)):\n",
    "\n",
    "        # since we require models of high variance,therefore taking max_depth as 30 which makes each model/decisiontree of deep \n",
    "        # depth due to which each model will have high variance and hence overfitted.We can also take greater value than 30.\n",
    "        regg = DecisionTreeRegressor(max_depth = 40) \n",
    "        regg.fit(samples[i],final_y_samples[i])\n",
    "        \n",
    "        new_x = x[:,selected_columns[i]]\n",
    "        y_pred_n = regg.predict(new_x) # predicting values\n",
    "        y_pred.append(y_pred_n)\n",
    "\n",
    "    \n",
    "    # https://www.geeksforgeeks.org/numpy-sum-in-python/\n",
    "    new_y_pred = np.sum(y_pred, dtype = np.float32,axis = 0) \n",
    "    new_y_pred[:] = [k /myInt for k in new_y_pred] # https://stackoverflow.com/questions/8244915/how-do-you-divide-each-element-in-a-list-by-an-int/8247234\n",
    "    \n",
    "    return new_y_pred\n",
    "\n",
    "def MSE(samples,myInt,final_y_samples,selected_columns):\n",
    "    m_y = []\n",
    "    new_y_pred = pred1(samples,myInt,final_y_samples,selected_columns)\n",
    "    s = 0\n",
    "    for i in range(len(y)):\n",
    "        s = s + (y[i] - new_y_pred[i]) ** 2 \n",
    "    d = 506\n",
    "    mse = s/d  \n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 4.646062193910587\n"
     ]
    }
   ],
   "source": [
    "myInt = 30\n",
    "mse = MSE(samples,myInt,final_y_samples,selected_columns)\n",
    "print(\"MSE:\",mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Step 3) Calculating the OOB score :-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred2(samples,myInt,final_y_samples,selected_columns):\n",
    "    final_y_pred = []\n",
    "    for r in x:\n",
    "        y_pred = 0\n",
    "        c = 0\n",
    "        for i in range(len(samples)):\n",
    "            new_r = [r[j] for j in selected_columns[i]]\n",
    "            \n",
    "            # checking if data point new_r form original dataset x is present in a given sample[i] or not\n",
    "            # and if it is not present then predicting it's value\n",
    "            if new_r not in samples[i]:\n",
    "                c = c + 1\n",
    "                regg = DecisionTreeRegressor(max_depth = 40)\n",
    "                regg.fit(samples[i],final_y_samples[i])\n",
    "                new_r = [new_r]\n",
    "                y_pred_n = regg.predict(new_r)\n",
    "                y_pred = y_pred + y_pred_n\n",
    "                \n",
    "        y_pred = y_pred/c\n",
    "        final_y_pred.append(y_pred)     \n",
    "        \n",
    "    return final_y_pred\n",
    "\n",
    "def OOB_Score(samples,myInt,final_y_samples,selected_columns):\n",
    "    final_y_pred = pred2(samples,myInt,final_y_samples,selected_columns)\n",
    "    su = 0\n",
    "    for i in range(len(y)):\n",
    "        su = su + (y[i] - final_y_pred[i]) ** 2\n",
    "    d = 506\n",
    "    oob_score = su/d\n",
    "    return oob_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB_Score: [28.4940629]\n"
     ]
    }
   ],
   "source": [
    "myInt = len(samples)\n",
    "oob_score = OOB_Score(samples,myInt,final_y_samples,selected_columns)\n",
    "print(\"OOB_Score:\",oob_score)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing CI of OOB Score and Train MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE_values(n1,n2,x):\n",
    "    mse_values = []\n",
    "    oob_scores = []\n",
    "    myInt = 30\n",
    "    for i in range(35):\n",
    "        samples,oob_samples,selected_columns,final_y_samples = creating_samples(n1,n2,x)\n",
    "        for i in range(len(samples)):\n",
    "            samples[i] = samples[i].tolist()\n",
    "        mse = MSE(samples,myInt,final_y_samples,selected_columns)\n",
    "        mse_values.append(mse)\n",
    "        oob = OOB_Score(samples,myInt,final_y_samples,selected_columns)\n",
    "        singleitem, = oob\n",
    "        [singleitem] = oob\n",
    "        oob_scores.append(singleitem)\n",
    "        \n",
    "    return mse_values,oob_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = 303\n",
    "n2 = 203\n",
    "mse_values,oob_scores = MSE_values(n1,n2,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "35\n",
      "[5.722601909744632, 4.691530411264451, 4.973040768111701, 4.064246109865387, 5.046751494719631, 4.819678175046227, 4.405033376777702, 4.991353027461776, 4.8656272672371195, 4.275467339347545, 5.327074211391696, 4.294457165130013, 5.345934184143025, 4.525599286616477, 3.9103473776404503, 4.279290067463021, 4.950414509885066, 5.004695458602769, 4.530081484989907, 4.465080808963959, 4.995705385953245, 5.2336253787728335, 4.442063552195678, 4.32648921695916, 4.7101447754273735, 5.128786932291537, 5.711927417392425, 4.273798227628697, 5.260516032493009, 3.8982385716244794, 5.019503371421606, 5.208218015270197, 4.789577500990751, 4.755427759106153, 5.043935705205433]\n",
      "[26.395387547487932, 22.57291544763647, 28.161243842513045, 27.796665791352133, 26.20857701292854, 28.086936334249717, 24.994551253458674, 25.332677354223097, 25.198964902931348, 26.035475956355928, 25.285178820919214, 27.62608018071837, 27.133503821519316, 29.301013515944454, 26.95040771405246, 24.619666856174888, 29.3252705323624, 26.41464706484284, 23.05894438066509, 24.612742462010534, 29.56150582780459, 25.19803918894754, 26.61984916370427, 27.059030126350915, 25.627968232540177, 23.208246387951757, 25.993762318463737, 26.441070045451042, 27.507547620055327, 24.85636246851355, 25.58355320753495, 26.770019852570915, 26.43549728649979, 25.452390411776626, 24.240426611781775]\n"
     ]
    }
   ],
   "source": [
    "print(len(mse_values))\n",
    "print(len(oob_scores))\n",
    "print(mse_values)\n",
    "print(oob_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.779607493632432\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean,stdev\n",
    "mse_mean = mean(mse_values)\n",
    "oob_mean = mean(oob_scores)\n",
    "print(mse_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_std = stdev(mse_values)\n",
    "oob_std = stdev(oob_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CI for MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "CI_left_limit_mse  = np.round(mse_mean - 2*(mse_std/np.sqrt(len(mse_values))), 3)\n",
    "CI_right_limit_mse = np.round(mse_mean + 2*(mse_std/np.sqrt(len(mse_values))), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CI for OOB score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "CI_left_limit_OOB_score = np.round(oob_mean - 2*(oob_std/np.sqrt(len(oob_scores))), 3)\n",
    "CI_right_limit_OOB_score = np.round(oob_mean + 2*(oob_std/np.sqrt(len(oob_scores))), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-------------+--------------------+---------------------------+----------+-----------+\n",
      "| #samples |   Metric  | Sample Size |    Sample mean     | Sample standard deviation | Left C.I | Right C.I |\n",
      "+----------+-----------+-------------+--------------------+---------------------------+----------+-----------+\n",
      "|    1     |    MSE    |      35     | 4.779607493632432  |     0.4626303951505117    |  4.623   |   4.936   |\n",
      "|    2     | OOB_Score |      35     | 26.161889129779812 |     1.6822820399334932    |  25.593  |   26.731  |\n",
      "+----------+-----------+-------------+--------------------+---------------------------+----------+-----------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "x = PrettyTable()\n",
    "x = PrettyTable([\"#samples\",\"Metric\", \"Sample Size\", \"Sample mean\", \"Sample standard deviation\",\"Left C.I\",\"Right C.I\"])\n",
    "x.add_row([1,'MSE',len(mse_values),mse_mean,mse_std,CI_left_limit_mse,CI_right_limit_mse])\n",
    "x.add_row([2,'OOB_Score',len(oob_scores),oob_mean,oob_std,CI_left_limit_OOB_score,CI_right_limit_OOB_score])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Given a single query point predict the price of house"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consider xq = [ 0.18 , 20.0 , 5.00 , 0.0 , 0.421 , 5.60 , 72.2 , 7.95 , 7.0 , 30.0 , 19.1 , 372.13 , 18.60 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "xq = [ 0.18 , 20.0 , 5.00 , 0.0 , 0.421 , 5.60 , 72.2 , 7.95 , 7.0 , 30.0 , 19.1 , 372.13 , 18.60 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_query_point(samples,selected_columns):\n",
    "    y_pred = 0\n",
    "    for i in range(len(samples)):\n",
    "        l = []\n",
    "        regg = DecisionTreeRegressor(max_depth = 30)\n",
    "        regg.fit(samples[i],y)\n",
    "        # getting columns for each sample which are then used to select columns from the query point for every iteration or for\n",
    "        # every sample so that only that columns/features of query point(xq) can be used in predicting the house price.\n",
    "        for i in selected_columns[i]:\n",
    "            l.append(xq[i])\n",
    "        l = [l] # since predict() takes argument in 2D form \n",
    "        \n",
    "        y_pred_n = regg.predict(l)\n",
    "        y_pred = y_pred + y_pred_n\n",
    "    y_pred = y_pred/30\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = pred_query_point(samples,selected_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "House price for xq point is : [24.13622222]\n"
     ]
    }
   ],
   "source": [
    "print(\"House price for xq point is :\",y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
